#include "detector.hpp"

namespace fs = filesystem;


MixDetector::MixDetector(shared_ptr<DataLoader> loader, 
                         const json& config)
    : Detector(loader, config) 
{
    const json& iforest_config_ = config_["detector"]["iforest_config"];
    random_seed_       = iforest_config_.value("random_seed", 2025);
    n_trees_           = iforest_config_.value("n_trees", 100);
    sample_size_       = iforest_config_.value("sample_size", 64);
    max_depth_         = iforest_config_.value("max_depth", 10);
    outline_threshold_ = iforest_config_.value("outline_threshold", 0.1);

    const json& dbstream_config = config_["detector"]["dbstream_config"];
    epsilon_      = dbstream_config.value("epsilon", 0.05);
    lambda_       = dbstream_config.value("lambda", 0.01);
    mu_           = dbstream_config.value("mu", 3);
    beta_merge_   = dbstream_config.value("beta_merge", 2.0);
    beta_noise_   = dbstream_config.value("beta_noise", 1.5);
    max_clusters_ = dbstream_config.value("max_clusters", 500);

    std::cout << "[MixDetector Initialized Parameters]\n";
    std::cout << "ğŸ”§ IForest Config:\n";
    std::cout << "  - random_seed        = " << random_seed_ << "\n";
    std::cout << "  - n_trees            = " << n_trees_ << "\n";
    std::cout << "  - sample_size        = " << sample_size_ << "\n";
    std::cout << "  - max_depth          = " << max_depth_ << "\n";
    std::cout << "  - outline_threshold  = " << outline_threshold_ << "\n";

    std::cout << "ğŸ§  DBSTREAM Config:\n";
    std::cout << "  - epsilon            = " << epsilon_ << "\n";
    std::cout << "  - lambda             = " << lambda_ << "\n";
    std::cout << "  - mu                 = " << mu_ << "\n";
    std::cout << "  - beta_merge         = " << beta_merge_ << "\n";
    std::cout << "  - beta_noise         = " << beta_noise_ << "\n";
    std::cout << "  - max_clusters       = " << max_clusters_ << "\n";
}

void MixDetector::Train(){

}

void MixDetector::Detect(){
    const auto& train_flows = *loader_->getTrainData();
    size_t total = train_flows.size();
    size_t count = 0;
    size_t print_interval = 1000;

    Time start_time = to_time_point(train_flows.front().first.ts_start);
    auto graphExtractor = std::make_unique<GraphFeatureExtractor>(config_, start_time);

    // === Step 1: æå–æ‰€æœ‰ç‰¹å¾å¹¶æ„å»ºæ•°æ®é›† ===
    std::vector<arma::vec> raw_samples;
    std::vector<double> timestamps;

    for (const auto &[flow, label] : train_flows) {
        graphExtractor->updateGraph(flow);
        arma::vec graphVec = graphExtractor->extract(flow);

        if (graphVec.is_empty()) continue;
        
        raw_samples.push_back(graphVec);
        timestamps.emplace_back(GET_DOUBLE_TS(flow.ts_start));

        if (++count % print_interval == 0 || count == total) {
            cout << "\rExtracting features... " << count << " / " << total << " samples." << flush;
        }
    }
    cout << endl; 

    if (raw_samples.empty()) {
        cout << "No valid samples extracted. Exiting.\n";
        return;
    }

    // === Step 2: æ•°æ®æ ‡å‡†åŒ– ===
    arma::mat flow_data(raw_samples[0].n_elem, raw_samples.size());
    for (size_t i = 0; i < raw_samples.size(); ++i)
        flow_data.col(i) = raw_samples[i];
    
    scaler_.Fit(flow_data);
    arma::mat norm_data;
    scaler_.Transform(flow_data, norm_data);

    // === Step 3: ä½¿ç”¨ä¿®å¤åçš„ DBSTREAM è¿›è¡Œèšç±» ===
    // ä½¿ç”¨æˆ‘ä»¬æ¨èçš„ã€ä¿®å¤åçš„å‚æ•°
    stream::DBSTREAM dbstream(epsilon_, lambda_, mu_, beta_merge_, beta_noise_, max_clusters_);

    size_t total_samples = norm_data.n_cols;
    cout << "Starting DBSTREAM clustering..." << endl;
    for (size_t i = 0; i < norm_data.n_cols; ++i) {
        dbstream.Insert(norm_data.col(i), timestamps[i]);

        if (i % print_interval == 0 || i == total_samples - 1) {
            double progress = (static_cast<double>(i + 1) / total_samples) * 100.0;
            cout << "\rClustering progress: " << i + 1 << " / " << total_samples 
                << " (" << std::fixed << std::setprecision(1) << progress << "%)" << flush;
        }
    }
    cout << endl; // åœ¨è¿›åº¦æ¡ç»“æŸåæ¢è¡Œ
    auto core_clusters = dbstream.GetClusters(timestamps.back());
    cout << "DBSTREAM found " << core_clusters.size() << " core clusters.\n";

    // âœ… ä¿®å¤1: ä½¿ç”¨ AllMicroClusters() è·å–æ‰€æœ‰å¾®ç°‡ï¼Œè€Œé GetClusters()
    // åŸå› ï¼šä¿®å¤åçš„ DBSTREAM ä¸­ï¼Œbeta_noise_ è¢«è®¾ç½®å¾—å¾ˆé«˜ï¼ˆå¦‚5.0ï¼‰ï¼Œç”¨äºæœ‰æ•ˆæ¸…ç†å†…å­˜ã€‚
    //       å› æ­¤ï¼ŒGetClusters() è¿”å›çš„åªæ˜¯è¡°å‡æƒé‡ >= mu_ çš„ç°‡ï¼Œè€Œè®¸å¤šâ€œå€™é€‰â€ä½†å°šæœªæˆç†Ÿçš„æ ¸å¿ƒç°‡ä¸ä¼šè¢«è¿”å›ã€‚
    //       æˆ‘ä»¬éœ€è¦æ£€æŸ¥æ‰€æœ‰ç°å­˜çš„å¾®ç°‡æ¥åˆ¤æ–­ä¸€ä¸ªç‚¹æ˜¯å¦â€œè¢«å¸æ”¶â€ã€‚
    auto all_micro_clusters = dbstream.AllMicroClusters(); // è·å–æ‰€æœ‰ç°å­˜çš„å¾®ç°‡
    double current_timestamp = timestamps.back();

    // === Step 4: åŸºäº DBSTREAM ç»“æœè¿›è¡Œå¼‚å¸¸æ£€æµ‹ ===
    unordered_set<size_t> outlier_ids;
    std::vector<int> assignments(norm_data.n_cols, -1); // -1 è¡¨ç¤ºæœªåˆ†é…ï¼ˆå³å¼‚å¸¸ï¼‰
    std::vector<double> distances_to_cluster(norm_data.n_cols, -1.0);

    // éå†æ¯ä¸ªæ•°æ®ç‚¹
    for (size_t i = 0; i < norm_data.n_cols; ++i) {
        const arma::vec& pt = norm_data.col(i);
        double best_dist = epsilon_; // ä½¿ç”¨ epsilon ä½œä¸ºå¸æ”¶åŠå¾„
        int best_idx = -1;

        // âœ… ä¿®å¤2: éå†æ‰€æœ‰å¾®ç°‡ï¼Œè€Œä¸ä»…ä»…æ˜¯â€œæ ¸å¿ƒâ€ç°‡
        // ç›®æ ‡ï¼šåˆ¤æ–­ç‚¹æ˜¯å¦è¢«ä»»ä½•ä¸€ä¸ªå¾®ç°‡åœ¨ epsilon é‚»åŸŸå†…å¸æ”¶ã€‚
        for (size_t j = 0; j < all_micro_clusters.size(); ++j) {
            const auto& mc = all_micro_clusters[j];
            double dist = arma::norm(pt - mc.center);
            if (dist < best_dist) {
                best_dist = dist;
                best_idx = static_cast<int>(j);
            }
        }

        if (best_idx >= 0) {
            // ç‚¹è¢«æŸä¸ªå¾®ç°‡å¸æ”¶
            assignments[i] = best_idx;
            distances_to_cluster[i] = best_dist;
        } else {
            // ç‚¹æœªè¢«ä»»ä½•å¾®ç°‡å¸æ”¶ â†’ ç›´æ¥æ ‡è®°ä¸ºå¼‚å¸¸
            outlier_ids.insert(i);
        }
    }

    // === Step 5: è¯†åˆ«â€œå¼±â€æˆ–â€œç¦»æ•£â€å¾®ç°‡ä¸­çš„å¼‚å¸¸ç‚¹ (å¯é€‰å¢å¼º) ===
    // è¿™ä¸ªæ­¥éª¤å¯ä»¥æ ¹æ®éœ€è¦ä¿ç•™ï¼Œç”¨äºå¤„ç†è¾¹ç•Œæƒ…å†µã€‚
    // ä½†è¯·æ³¨æ„ï¼Œç”±äº beta_noise_ è®¾ç½®å¾—è¾ƒé«˜ï¼Œå¾ˆå¤šâ€œå¼±â€ç°‡åœ¨è¢«è¯†åˆ«å‰å°±å·²è¢«æ¸…ç†ã€‚
    // å› æ­¤ï¼Œè¿™ä¸€æ­¥çš„å¿…è¦æ€§é™ä½ï¼Œä½†å¯ä»¥ä½œä¸ºäºŒæ¬¡è¿‡æ»¤ã€‚

    const double cluster_dispersion_threshold = 0.5; 
    const double cluster_strength_threshold = mu_ * 1.5; 

    for (size_t cid = 0; cid < all_micro_clusters.size(); ++cid) {
        const auto& cluster = all_micro_clusters[cid];
        double decayed_weight = dbstream.QueryDecayedWeight(cluster, current_timestamp);
        
        std::vector<std::pair<size_t, double>> points_in_cluster;
        for (size_t i = 0; i < assignments.size(); ++i) {
            if (assignments[i] == static_cast<int>(cid)) {
                points_in_cluster.emplace_back(i, distances_to_cluster[i]);
            }
        }

        if (points_in_cluster.size() < 2) continue; // è‡³å°‘æœ‰ä¸¤ä¸ªç‚¹æ‰è®¡ç®—ç¦»æ•£åº¦

        double total_dist = 0.0;
        for (const auto& [pid, dist] : points_in_cluster) total_dist += dist;
        double avg_dist = total_dist / points_in_cluster.size();

        // âœ… ä¿®å¤3: è°ƒæ•´é€»è¾‘ã€‚å¦‚æœç°‡å¾ˆå¼±ï¼Œä½†ç‚¹éƒ½åœ¨ä¸­å¿ƒï¼Œå¯èƒ½æ²¡é—®é¢˜ã€‚
        //          å¦‚æœç°‡å¾ˆç¦»æ•£ï¼Œå³ä½¿å¾ˆå¼ºï¼Œä¹Ÿå¯èƒ½æœ‰é—®é¢˜ã€‚
        //          è¿™é‡Œæˆ‘ä»¬æ›´å…³æ³¨â€œç¦»æ•£åº¦â€ï¼Œå› ä¸ºå®ƒæ›´èƒ½åæ˜ å†…éƒ¨çš„ä¸€è‡´æ€§ã€‚
        if (avg_dist > cluster_dispersion_threshold) {
            auto max_it = std::max_element(points_in_cluster.begin(), points_in_cluster.end(),
                [](const auto& a, const auto& b) { return a.second < b.second; });
            // âœ… ä¼˜åŒ–: åªæ ‡è®°æœ€è¿œçš„1-2ä¸ªç‚¹ï¼Œè€Œä¸æ˜¯æ•´ä¸ªç°‡
            outlier_ids.insert(max_it->first);
            // å¦‚æœæƒ³æ›´æ¿€è¿›ï¼Œå¯ä»¥æ ‡è®°è·ç¦»å¤§äº avg_dist + std_dev çš„æ‰€æœ‰ç‚¹
        }
    }

    // === Step 6: è¯„ä¼°ä¸è¾“å‡º ===
    fs::create_directory("result");
    std::string output_file = "result/dbstream_predict_center.csv";
    std::ofstream ofs(output_file);
    ofs << "SrcIP,DstIP,Dist,Pred,Label\n";

    size_t TP = 0, TN = 0, FP = 0, FN = 0;
    for (size_t i = 0; i < train_flows.size(); ++i) {
        const auto& [flow, label] = train_flows[i];
        double dist = distances_to_cluster[i];
        size_t pred = outlier_ids.count(i) ? 1 : 0;

        if (pred == 1 && label == 1) TP++;
        else if (pred == 0 && label == 0) TN++;
        else if (pred == 1 && label == 0) FP++;
        else if (pred == 0 && label == 1) FN++;

        ofs << flow.src_ip << "," << flow.dst_ip << "," << dist << "," << pred << "," << label << "\n";
    }
    ofs.close();

    double accuracy = (double)(TP + TN) / (TP + TN + FP + FN);
    double precision = TP + FP ? (double)TP / (TP + FP) : 0.0;
    double recall = TP + FN ? (double)TP / (TP + FN) : 0.0;
    double fpr = (FP + TN) ? (double)FP / (FP + TN) : 0.0;
    double f1 = (precision + recall) ? 2 * precision * recall / (precision + recall) : 0.0;

    cout << "ğŸ“Š Final Evaluation (DBSTREAM):\n";
    cout << "âœ… Accuracy  : " << accuracy * 100 << "%\n";
    cout << "ğŸ¯ Precision : " << precision * 100 << "%\n";
    cout << "ğŸ“¥ Recall    : " << recall * 100 << "%\n";
    cout << "ğŸš¨ FPR       : " << fpr * 100 << "%\n";
    cout << "ğŸ“ˆ F1-Score  : " << f1 * 100 << "%\n";
    cout << "ğŸ“ Results written to: " << output_file << "\n";

    // === é™ç»´å¯è§†åŒ–è¾“å‡º ===
    mlpack::PCA pca;
    arma::mat reduced;
    pca.Apply(norm_data, reduced, 2);

    std::ofstream fout("result/dbstream_pca_result.csv");
    fout << "x,y,assignments,label,is_outlier\n";
    for (size_t i = 0; i < reduced.n_cols; ++i) {
        const size_t label = train_flows[i].second;
        bool is_outlier = outlier_ids.count(i) > 0;
        int assign = assignments[i];
        fout << reduced(0, i) << "," << reduced(1, i) << "," << assign << "," << label << "," << is_outlier << "\n";
    }
    fout.close();
}

void MixDetector::run() {
    Detect();
}